{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF","metadata":{"execution":{"iopub.status.busy":"2023-07-20T15:21:32.168898Z","iopub.execute_input":"2023-07-20T15:21:32.170049Z","iopub.status.idle":"2023-07-20T15:21:32.175608Z","shell.execute_reply.started":"2023-07-20T15:21:32.169997Z","shell.execute_reply":"2023-07-20T15:21:32.174442Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        #Kernel 3, stride 1, padding 1, bias to false due to batch normalization\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T15:40:34.550581Z","iopub.execute_input":"2023-07-20T15:40:34.550996Z","iopub.status.idle":"2023-07-20T15:40:34.559903Z","shell.execute_reply.started":"2023-07-20T15:40:34.550954Z","shell.execute_reply":"2023-07-20T15:40:34.558710Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class UNET(nn.Module):\n    def __init__(\n            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n    ):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T15:40:37.152278Z","iopub.execute_input":"2023-07-20T15:40:37.152734Z","iopub.status.idle":"2023-07-20T15:40:37.167828Z","shell.execute_reply.started":"2023-07-20T15:40:37.152698Z","shell.execute_reply":"2023-07-20T15:40:37.166837Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def test():\n    x = torch.randn((3,1, 160, 160))\n    model = UNET(in_channels = 1, out_channels = 1)\n    preds = model(x)\n    print(preds.shape)\n    print(x.shape)\n    assert preds.shape == x.shape\n    \nif __name__ == \"__main__\":\n    test()","metadata":{"execution":{"iopub.status.busy":"2023-07-20T15:40:39.941968Z","iopub.execute_input":"2023-07-20T15:40:39.942520Z","iopub.status.idle":"2023-07-20T15:40:41.725502Z","shell.execute_reply.started":"2023-07-20T15:40:39.942473Z","shell.execute_reply":"2023-07-20T15:40:41.724283Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"torch.Size([3, 1, 160, 160])\ntorch.Size([3, 1, 160, 160])\n","output_type":"stream"}]}]}